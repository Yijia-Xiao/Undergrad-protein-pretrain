# Undergrad-protein-pretrain

**A talk targeted at junior undergraduates organized by the KEG group**

Materials for `Protein Pre-training Model`



## Background
Protein is related to almost every life process. Therefore, analyzing the biological structure and characteristics of protein sequences is essential for exploring life, disease detection, and drug discovery. Traditional protein analysis methods are often labor-intensive and time-consuming. The emergence of deep learning models makes it possible to model data patterns in large amounts of data. Interdisciplinary researchers have begun to use deep learning methods to model large biological data sets, such as the use of long and short-term memory and convolutional neural networks for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Large-scale pre-trained language models can be used to extract these types of information.


## Releted Works
### TAPE
- Paper [Evaluating Protein Transfer Learning with TAPE](https://arxiv.org/abs/1906.08230)
- Code [TAPE](https://github.com/songlab-cal/tape)

### ProteinLM
- Paper [Modeling Protein Using Large-scale Pretrain Language Model](https://arxiv.org/abs/2108.07435)
- Code [ProteinLM](https://github.com/THUDM/ProteinLM)


### ProtTrans
- Paper [ProtTrans: cracking the language of lifeâ€™s code through self-supervised deep learning and high performance computing](https://arxiv.org/abs/2007.06225)
- Code [ProtTrans](https://github.com/agemagician/ProtTrans)


### ESM
- Paper [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.biorxiv.org/content/10.1101/622803v4)
- Code [ESM](https://github.com/facebookresearch/esm)


